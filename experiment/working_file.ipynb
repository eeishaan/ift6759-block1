{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_memfile(filename, shape, dtype='float32'):\n",
    "    # read binary data and return as a numpy array\n",
    "    fp = np.memmap(filename, dtype=dtype, mode='r', shape=shape)\n",
    "    data = np.zeros(shape=shape, dtype=dtype)\n",
    "    data[:] = fp[:]\n",
    "    del fp\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memfile(data, filename):\n",
    "    # write a numpy array 'data' into a binary  data file specified by\n",
    "    # 'filename'\n",
    "    shape = data.shape\n",
    "    dtype = data.dtype\n",
    "    fp = np.memmap(filename, dtype=dtype, mode='w+', shape=shape)\n",
    "    fp[:] = data[:]\n",
    "    del fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/rap/jvb-000-aa/COURS2019/etudiants/data/omsignal/myHeartProject/'\n",
    "TRAIN_LABELED_FILE = 'MILA_TrainLabeledData.dat'\n",
    "VALIDATION_LABELED_FILE =  'MILA_ValidationLabeledData.dat'\n",
    "\n",
    "TRAIN_LABELED_FILE = 'MILA_TrainLabeledData.dat'\n",
    "VALIDATION_LABELED_FILE = 'MILA_ValidationLabeledData.dat'\n",
    "LABELS_NAME = [\"PR_Mean\", \"RT_Mean\", \"RR_StdDev\", \"ID\"]\n",
    "\n",
    "train_labeled_data_file = os.path.join(ROOT_DIR, TRAIN_LABELED_FILE)\n",
    "validation_labeled_data_file = os.path.join(ROOT_DIR, VALIDATION_LABELED_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatrain = read_memfile(train_labeled_data_file, shape=(160,3754))\n",
    "train_data, train_labels = datatrain[:,:-4], datatrain[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "datavalid = read_memfile(validation_labeled_data_file, shape=(160,3754))\n",
    "valid_data, valid_labels = datavalid[:,:-4], datavalid[:,-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            ma_window_size=2,\n",
    "            mv_window_size=4,\n",
    "            num_samples_per_second=125):\n",
    "        # ma_window_size: (in seconds) window size to use\n",
    "        #                 for moving average baseline wander removal\n",
    "        # mv_window_size: (in seconds) window size to use\n",
    "        #                 for moving average RMS normalization\n",
    "\n",
    "        super(Preprocessor, self).__init__()\n",
    "\n",
    "        # Kernel size to use for moving average baseline wander removal: 2\n",
    "        # seconds * 125 HZ sampling rate, + 1 to make it odd\n",
    "\n",
    "        self.maKernelSize = (ma_window_size * num_samples_per_second) + 1\n",
    "\n",
    "        # Kernel size to use for moving average normalization: 4\n",
    "        # seconds * 125 HZ sampling rate , + 1 to make it odd\n",
    "\n",
    "        self.mvKernelSize = (mv_window_size * num_samples_per_second) + 1\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "\n",
    "            # Remove window mean and standard deviation\n",
    "\n",
    "            x = (x - torch.mean(x, dim=2, keepdim=True)) / \\\n",
    "                (torch.std(x, dim=2, keepdim=True) + 0.00001)\n",
    "\n",
    "            # Moving average baseline wander removal\n",
    "\n",
    "            x = x - F.avg_pool1d(\n",
    "                x, kernel_size=self.maKernelSize,\n",
    "                stride=1, padding=(self.maKernelSize - 1) // 2\n",
    "            )\n",
    "\n",
    "            # Moving RMS normalization\n",
    "\n",
    "            x = x / (\n",
    "                torch.sqrt(\n",
    "                    F.avg_pool1d(\n",
    "                        torch.pow(x, 2),\n",
    "                        kernel_size=self.mvKernelSize,\n",
    "                        stride=1, padding=(self.mvKernelSize - 1) // 2\n",
    "                    )) + 0.00001\n",
    "            )\n",
    "\n",
    "        # Don't backpropagate further\n",
    "\n",
    "        x = x.detach().contiguous()\n",
    "\n",
    "        return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_ECG(data, start=0, end=30, index_nb=[0], seconde=True):\n",
    "    \"\"\"\n",
    "    Plot the ECG on a user defined time interval\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy array\n",
    "        The ECG Data\n",
    "    start: int\n",
    "        The starting time of the part of the ECG you want to plot\n",
    "    end: int\n",
    "        The End time of the part of the ECG you want to plot\n",
    "    index_nb: List\n",
    "        The index of the observation you want to plot\n",
    "    seconde: boolean\n",
    "        Used True if the start time is in seconde\n",
    "    \"\"\"\n",
    "    # Adjust the shape in order to have 2 dimension\n",
    "    if len(data.shape) == 1:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "        \n",
    "    # Transform the start and end time \n",
    "    if seconde == True:\n",
    "        start = start*125\n",
    "        end = end*125\n",
    "        \n",
    "    x_labels = np.arange(start, end)\n",
    "             \n",
    "    for i, ex in enumerate(index_nb):\n",
    "        plt.figure(i*2+1) \n",
    "        plt.plot(x_labels, data[ex, start:end])\n",
    "        \n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class id_trasformer:\n",
    "    def __init__(self):\n",
    "        self.DictId2Class = {}\n",
    "        self.DictClass2Id = {}\n",
    "        self.n_Id = 0 \n",
    "        \n",
    "    def create_dict_ID(self, ID_vector):\n",
    "        for id in ID_vector:\n",
    "            if id not in self.DictId2Class:\n",
    "                self.DictId2Class[id] = self.n_Id\n",
    "                self.DictClass2Id[self.n_Id] = id\n",
    "                self.n_Id += 1\n",
    "                \n",
    "    def Id2Class(self, ID_vector):\n",
    "        return [self.DictId2Class[id] for id in ID_vector]\n",
    "    \n",
    "    def Class2Id(self, class_vector):\n",
    "        return [self.DictClass2Id[labels] for labels in class_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.tensor(train_data).to(device)\n",
    "valid =  torch.tensor(valid_data).to(device)\n",
    "preprocess_train = preprocessor(train)\n",
    "preprocess_valid = preprocessor(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "idtransformer = id_trasformer()\n",
    "idtransformer.create_dict_ID(train_labels[:,-1])\n",
    "train_labels[:,-1] = idtransformer.Id2Class(train_labels[:,-1])\n",
    "valid_labels[:,-1] = idtransformer.Id2Class(valid_labels[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo to detect R P,T peak and onset offset based on https://pure.tugraz.at/ws/portalfiles/portal/1312717/Online%20and%20Offline%20Determination%20of%20QT%20PR%20in%20Electrocardiography%20LNCS%207719.pdfdf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_R_peak(data, SADA_wd_size = 7, FS_wd_size = 12, Threshold = 35):\n",
    "    \"\"\"\n",
    "    Take a Batch of ECG data and find the location of the R Peak\n",
    "    \n",
    "    The algorithm is based on the paper:\n",
    "    Online and Offline Determination of QT and PR Interval and QRS Duration in Electrocardiography\n",
    "    (Bachler et al., 2012)\n",
    "    The variable name and default value follow the paper\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy array\n",
    "        The ECG Data (batch size x lenght of the ECG recording)\n",
    "    SADA_wd_size: int\n",
    "        size of the moving window used in the calculation of SA and DA\n",
    "    FS_wd_size: int\n",
    "        size of the moving window used in the calculation of the feature signal FS\n",
    "    Threshold: int\n",
    "        FS is compared to the Threshold to determined if its a QRS zone. \n",
    "    \"\"\"\n",
    "    \n",
    "    R_peak = []\n",
    "    \n",
    "    #Allow batch size of 1\n",
    "    if len(data.size()) == 1:\n",
    "        data = data.unsqueeze(0)\n",
    "    \n",
    "    D = data[:, 1:] - data[:, 0:-1]\n",
    "    \n",
    "    \n",
    "    data = data.unsqueeze(0)\n",
    "    D = D.unsqueeze(0)\n",
    "    SA = F.max_pool1d(data, kernel_size = SADA_wd_size, stride = 1)\n",
    "    SA = SA + F.max_pool1d(-data, kernel_size = SADA_wd_size, stride = 1) \n",
    "    DA = F.max_pool1d(D, kernel_size = SADA_wd_size, stride = 1, padding=1)\n",
    "    DA = DA + F.max_pool1d(-D, kernel_size = SADA_wd_size, stride = 1, padding=1) \n",
    "    \n",
    "    C = DA[:,:,1:] * torch.pow(SA, 2)\n",
    "    FS = F.max_pool1d(C, kernel_size = FS_wd_size, stride = 1) \n",
    "    Detect = (FS > Threshold)\n",
    "    \n",
    "    Detect = Detect.squeeze(0).cpu()\n",
    "    data = data.squeeze(0).cpu()\n",
    "\n",
    "    for ECG in range(len(data)):\n",
    "        \n",
    "        in_QRS = 0\n",
    "        start_QRS = 0\n",
    "        end_QRS = 0\n",
    "        r_peak = np.array([])\n",
    "        \n",
    "        for tick, detect in enumerate(Detect[ECG]):\n",
    "            \n",
    "            if (in_QRS == 0) and (detect == 1):\n",
    "                start_QRS = tick\n",
    "                in_QRS = 1\n",
    "                \n",
    "            elif (in_QRS == 1) and (detect == 0):\n",
    "                end_QRS = tick\n",
    "                R_tick = torch.argmax(data[ECG, start_QRS : end_QRS+SADA_wd_size+FS_wd_size]).item()\n",
    "                r_peak = np.append(r_peak, R_tick + start_QRS)\n",
    "                in_QRS = 0\n",
    "                start_QRS = 0\n",
    "                \n",
    "        R_peak.append(r_peak)\n",
    "        \n",
    "    return R_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_peak_train = detect_R_peak(preprocess_train)\n",
    "R_peak_valid = detect_R_peak(preprocess_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_RR_Mean_std(R_peak, MaxInterval=180):\n",
    "    \"\"\"\n",
    "    Calculate the mean RR interval and the std\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    R_peak : list of list\n",
    "        Each entry is a list of the positiion of the R peak in the ECG\n",
    "    MaxInterval: int\n",
    "        maximum lenght of an interval, interval higher than this amount are ignore\n",
    "    \"\"\"\n",
    "    #calculate the lenght of the interval\n",
    "    RR_interval = [R_peak[i][1:]-R_peak[i][0:-1] for i in range(len(R_peak))]\n",
    "    \n",
    "    #We keep only good quality one\n",
    "    RR_interval_adj = [interval[interval<180] for interval in RR_interval]\n",
    "    \n",
    "\n",
    "    RR_interval_mean = [np.mean(interval) for interval in RR_interval_adj]    \n",
    "    RR_std = [np.std(interval) for interval in RR_interval_adj]\n",
    "    \n",
    "    return RR_interval_mean, RR_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_RR_mean, Train_RR_std = Get_RR_Mean_std(R_peak_train)\n",
    "Valid_RR_mean, Valid_RR_std = Get_RR_Mean_std(R_peak_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(data, R_peak, template_size = 110, All_window = True):\n",
    "    \n",
    "    listoftemplate1 = []\n",
    "    listoftemplate2 = []\n",
    "    half_size_int = template_size//2\n",
    "    listECG_id = []\n",
    "    \n",
    "    for recording in range(len(data)):\n",
    "        template = []\n",
    "        ECG_id = []\n",
    "        #generate the template      \n",
    "        \n",
    "        for i in R_peak[recording][1:-1]:\n",
    "            new_heart_beat = data[recording][int(i)-int(half_size_int*0.8): int(i)+int(half_size_int*1.2)]\n",
    "            if len(new_heart_beat) != 110:\n",
    "                print(\"error\")\n",
    "            template.append(new_heart_beat)\n",
    "            \n",
    "            if All_window:\n",
    "                ECG_id.append(recording)        \n",
    "\n",
    "        if All_window == False:\n",
    "            template = np.mean(template, axis = 0)\n",
    "            template = np.expand_dims(template, axis = 0)\n",
    "            ECG_id.append(recording)  \n",
    "            \n",
    "        listoftemplate1 = listoftemplate1 + template\n",
    "        listECG_id = listECG_id + ECG_id\n",
    "        \n",
    "        \n",
    "    #listoftemplate = [hearth_beat for hearth_beat in template for template in listoftemplate]\n",
    "    return np.array(listoftemplate1), np.array(listECG_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template_average(data, R_peak, template_size = 110):\n",
    "    half_size_int = template_size//2\n",
    "    listoftemplate = []\n",
    "    for recording in range(len(R_peak)):\n",
    "        #generate the template\n",
    "        template = np.zeros((1,int(half_size_int*0.8)+int(half_size_int*1.2)))\n",
    "        for i in R_peak[recording][1:-1]:\n",
    "            new_heart_beat = data[recording][int(i)-int(half_size_int*0.8): int(i)+int(half_size_int*1.2)]\n",
    "            template = np.concatenate((template,\n",
    "                                       np.expand_dims(new_heart_beat, axis = 0))\n",
    "                                       , axis=0)\n",
    "\n",
    "\n",
    "        template = np.mean(template, axis = 0)\n",
    "        listoftemplate.append(template)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #listoftemplate = [hearth_beat for hearth_beat in template for template in listoftemplate]\n",
    "    return listoftemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we should probably combine the two function (create_template,create_template_average) above into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_valid, t_valid_labels = create_template(preprocess_valid.cpu().numpy(),  R_peak_valid)\n",
    "template_train, t_train_labels = create_template(preprocess_train.cpu().numpy(), R_peak_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Avg_template_valid = create_template_average(preprocess_valid.cpu().numpy(),  R_peak_valid)\n",
    "Avg_template_train = create_template_average(preprocess_train.cpu().numpy(), R_peak_train)\n",
    "Avg_template_train = np.array(Avg_template_train)\n",
    "Avg_template_valid = np.array(Avg_template_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution for classification on all window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models,transforms,datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import copy\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictECG_id_to_labels1 = {i:j for i, j in enumerate(train_labels[:,-1])}\n",
    "dictECG_id_to_labels2 = {i:j for i, j in enumerate(valid_labels[:,-1])}\n",
    "labels_train = np.array([dictECG_id_to_labels1[i] for i in t_train_labels])\n",
    "labels_valid = np.array([dictECG_id_to_labels2[i] for i in t_valid_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.Tensor(template_train)\n",
    "labels_train_t = torch.LongTensor(labels_train)\n",
    "trainloader = torch.utils.data.TensorDataset(data_train, labels_train_t)\n",
    "loader_train = torch.utils.data.DataLoader(trainloader, batch_size=128, shuffle = True)\n",
    "\n",
    "data_valid = torch.Tensor(template_valid)\n",
    "labels_valid_t = torch.LongTensor(labels_valid)\n",
    "validloader = torch.utils.data.TensorDataset(data_valid, labels_valid_t)\n",
    "loader_valid = torch.utils.data.DataLoader(validloader, batch_size=128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,10,kernel_size=5) #22\n",
    "        self.conv2=nn.Conv1d(10,20,kernel_size=5) #18\n",
    "        self.drop_conv2=nn.Dropout2d(p=0.5) #4\n",
    "        self.fc1=nn.Linear(60,50)\n",
    "        self.drop_lin=nn.Dropout(p=0.5)\n",
    "        self.fc2=nn.Linear(50,32)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x=F.relu(F.max_pool1d(self.conv1(x),5))\n",
    "        x=F.relu(F.max_pool1d(self.drop_conv2(self.conv2(x)),5))\n",
    "        x=x.view(-1,60)\n",
    "        x=F.relu(self.drop_lin(self.fc1(x)))\n",
    "        x=F.log_softmax(self.fc2(x), dim = 1)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, lr = 0.1):\n",
    "  \n",
    "    optmizer=optim.SGD(model.parameters(),lr=lr)\n",
    "    best_model=best_model=copy.deepcopy(model)\n",
    "    best_accuracy=0\n",
    "    best_epoch=0\n",
    "  \n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        for mode in [\"train\",\"eval\"]:\n",
    "\n",
    "            accuracy=0\n",
    "            loss_total=0\n",
    "\n",
    "            if mode == \"train\":\n",
    "\n",
    "                model.train()\n",
    "                for data, labels in loader_train:\n",
    "                    data, labels=data.to(device), labels.to(device)\n",
    "                    optmizer.zero_grad()\n",
    "\n",
    "                    output=model(data)\n",
    "                    pred=torch.argmax(output,1)\n",
    "                    loss=F.nll_loss(output, labels)\n",
    "\n",
    "                    accuracy+=(pred==labels).sum()\n",
    "                    loss_total+=loss\n",
    "\n",
    "                    loss.backward()\n",
    "                    optmizer.step()\n",
    "\n",
    "                if i % 25 == 0:\n",
    "                    print(f\"Epoch{i} Training loss: {loss_total.item()}\")\n",
    "                    print(f\"Training Accuracy:{accuracy.item()/len(loader_train.dataset)}\")\n",
    "\n",
    "            if mode == \"eval\":\n",
    "\n",
    "                model.eval()\n",
    "                for data, labels in loader_valid:\n",
    "                    data, labels=data.to(device), labels.to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        optmizer.zero_grad()\n",
    "\n",
    "                        output=model(data)\n",
    "                        pred=torch.argmax(output,1)\n",
    "                        loss=F.nll_loss(output, labels)\n",
    "\n",
    "                        accuracy+=(pred==labels).sum()\n",
    "                        loss_total+=loss\n",
    "\n",
    "                accuracy=accuracy.item()/len(loader_valid.dataset)\n",
    "\n",
    "                if i % 25 == 0:\n",
    "                    print(f\"Eval loss: {loss_total.item()}\")\n",
    "                    print(f\"Eval Accuracy:{accuracy}\")\n",
    "\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy=accuracy\n",
    "                    best_model=copy.deepcopy(model)\n",
    "                    best_epoch=i\n",
    "\n",
    "    print(f\"The best epoch:{best_epoch} with an accuracy of {best_accuracy}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 Training loss: 138.79293823242188\n",
      "Training Accuracy:0.03467980295566502\n",
      "Eval loss: 137.66673278808594\n",
      "Eval Accuracy:0.03402041224734841\n",
      "Epoch25 Training loss: 62.71392059326172\n",
      "Training Accuracy:0.483743842364532\n",
      "Eval loss: 113.14208984375\n",
      "Eval Accuracy:0.4006403842305383\n",
      "Epoch50 Training loss: 48.044490814208984\n",
      "Training Accuracy:0.6041379310344828\n",
      "Eval loss: 114.03598022460938\n",
      "Eval Accuracy:0.45267160296177705\n",
      "Epoch75 Training loss: 43.2799186706543\n",
      "Training Accuracy:0.6364532019704433\n",
      "Eval loss: 115.51734161376953\n",
      "Eval Accuracy:0.5157094256553932\n",
      "Epoch100 Training loss: 41.2182502746582\n",
      "Training Accuracy:0.6510344827586206\n",
      "Eval loss: 108.62220001220703\n",
      "Eval Accuracy:0.5229137482489493\n",
      "Epoch125 Training loss: 40.48299789428711\n",
      "Training Accuracy:0.6711330049261084\n",
      "Eval loss: 104.4974594116211\n",
      "Eval Accuracy:0.42525515309185513\n",
      "Epoch150 Training loss: 34.27692794799805\n",
      "Training Accuracy:0.7152709359605911\n",
      "Eval loss: 100.2510757446289\n",
      "Eval Accuracy:0.5295177106263759\n",
      "Epoch175 Training loss: 32.743927001953125\n",
      "Training Accuracy:0.7324137931034482\n",
      "Eval loss: 103.8792953491211\n",
      "Eval Accuracy:0.511106663998399\n",
      "Epoch200 Training loss: 33.959712982177734\n",
      "Training Accuracy:0.7257142857142858\n",
      "Eval loss: 99.5185546875\n",
      "Eval Accuracy:0.5437262357414449\n",
      "Epoch225 Training loss: 31.72449493408203\n",
      "Training Accuracy:0.7318226600985221\n",
      "Eval loss: 94.41719055175781\n",
      "Eval Accuracy:0.5371222733640184\n",
      "Epoch250 Training loss: 30.736604690551758\n",
      "Training Accuracy:0.7399014778325124\n",
      "Eval loss: 117.18766784667969\n",
      "Eval Accuracy:0.4890934560736442\n",
      "Epoch275 Training loss: 28.740095138549805\n",
      "Training Accuracy:0.7550738916256158\n",
      "Eval loss: 99.55382537841797\n",
      "Eval Accuracy:0.5815489293576146\n",
      "Epoch300 Training loss: 29.487083435058594\n",
      "Training Accuracy:0.7564532019704433\n",
      "Eval loss: 102.98527526855469\n",
      "Eval Accuracy:0.5825495297178307\n",
      "Epoch325 Training loss: 28.147722244262695\n",
      "Training Accuracy:0.7645320197044335\n",
      "Eval loss: 101.12507629394531\n",
      "Eval Accuracy:0.573143886331799\n",
      "Epoch350 Training loss: 29.95294952392578\n",
      "Training Accuracy:0.7501477832512315\n",
      "Eval loss: 97.15200805664062\n",
      "Eval Accuracy:0.5759455673404043\n",
      "Epoch375 Training loss: 29.908193588256836\n",
      "Training Accuracy:0.7548768472906404\n",
      "Eval loss: 93.92220306396484\n",
      "Eval Accuracy:0.5965579347608565\n",
      "Epoch400 Training loss: 28.735300064086914\n",
      "Training Accuracy:0.7582266009852217\n",
      "Eval loss: 101.16034698486328\n",
      "Eval Accuracy:0.5883530118070842\n",
      "Epoch425 Training loss: 27.76664161682129\n",
      "Training Accuracy:0.7676847290640394\n",
      "Eval loss: 100.18632507324219\n",
      "Eval Accuracy:0.5931558935361216\n",
      "Epoch450 Training loss: 26.18821144104004\n",
      "Training Accuracy:0.7808866995073892\n",
      "Eval loss: 98.91606903076172\n",
      "Eval Accuracy:0.5953572143285971\n",
      "Epoch475 Training loss: 26.35293960571289\n",
      "Training Accuracy:0.7808866995073892\n",
      "Eval loss: 100.09066772460938\n",
      "Eval Accuracy:0.566940164098459\n",
      "Epoch500 Training loss: 28.027097702026367\n",
      "Training Accuracy:0.7678817733990148\n",
      "Eval loss: 107.46965026855469\n",
      "Eval Accuracy:0.5871522913748249\n",
      "Epoch525 Training loss: 26.390565872192383\n",
      "Training Accuracy:0.7822660098522167\n",
      "Eval loss: 107.77430725097656\n",
      "Eval Accuracy:0.5833500100060036\n",
      "Epoch550 Training loss: 25.753929138183594\n",
      "Training Accuracy:0.788768472906404\n",
      "Eval loss: 110.94752502441406\n",
      "Eval Accuracy:0.5737442465479288\n",
      "Epoch575 Training loss: 25.526275634765625\n",
      "Training Accuracy:0.7824630541871921\n",
      "Eval loss: 111.27088928222656\n",
      "Eval Accuracy:0.5821492895737442\n",
      "Epoch600 Training loss: 24.58602523803711\n",
      "Training Accuracy:0.7903448275862069\n",
      "Eval loss: 101.01139068603516\n",
      "Eval Accuracy:0.588953372023214\n",
      "Epoch625 Training loss: 25.218114852905273\n",
      "Training Accuracy:0.7952709359605912\n",
      "Eval loss: 108.41575622558594\n",
      "Eval Accuracy:0.590754452671603\n",
      "Epoch650 Training loss: 26.40910530090332\n",
      "Training Accuracy:0.7783251231527094\n",
      "Eval loss: 108.91783905029297\n",
      "Eval Accuracy:0.5951570942565539\n",
      "Epoch675 Training loss: 25.232402801513672\n",
      "Training Accuracy:0.7901477832512315\n",
      "Eval loss: 105.6086196899414\n",
      "Eval Accuracy:0.600560336201721\n",
      "Epoch700 Training loss: 24.24321937561035\n",
      "Training Accuracy:0.7933004926108375\n",
      "Eval loss: 117.38719177246094\n",
      "Eval Accuracy:0.6109665799479688\n",
      "Epoch725 Training loss: 23.59002113342285\n",
      "Training Accuracy:0.8043349753694581\n",
      "Eval loss: 116.00651550292969\n",
      "Eval Accuracy:0.5679407644586753\n",
      "Epoch750 Training loss: 24.218883514404297\n",
      "Training Accuracy:0.8003940886699508\n",
      "Eval loss: 102.79707336425781\n",
      "Eval Accuracy:0.6085651390834501\n",
      "Epoch775 Training loss: 24.58587074279785\n",
      "Training Accuracy:0.7931034482758621\n",
      "Eval loss: 104.59855651855469\n",
      "Eval Accuracy:0.5997598559135481\n",
      "Epoch800 Training loss: 26.01082420349121\n",
      "Training Accuracy:0.7934975369458128\n",
      "Eval loss: 128.97145080566406\n",
      "Eval Accuracy:0.563338002801681\n",
      "Epoch825 Training loss: 24.300729751586914\n",
      "Training Accuracy:0.8\n",
      "Eval loss: 112.33097839355469\n",
      "Eval Accuracy:0.5735441264758855\n",
      "Epoch850 Training loss: 24.313405990600586\n",
      "Training Accuracy:0.8051231527093596\n",
      "Eval loss: 99.53421020507812\n",
      "Eval Accuracy:0.604962977786672\n",
      "Epoch875 Training loss: 27.830562591552734\n",
      "Training Accuracy:0.781871921182266\n",
      "Eval loss: 115.07953643798828\n",
      "Eval Accuracy:0.5827496497898739\n",
      "Epoch900 Training loss: 23.416261672973633\n",
      "Training Accuracy:0.8102463054187192\n",
      "Eval loss: 112.23368072509766\n",
      "Eval Accuracy:0.5899539723834301\n",
      "Epoch925 Training loss: 24.198022842407227\n",
      "Training Accuracy:0.8021674876847291\n",
      "Eval loss: 116.82671356201172\n",
      "Eval Accuracy:0.5781468881328797\n",
      "Epoch950 Training loss: 23.786848068237305\n",
      "Training Accuracy:0.8041379310344827\n",
      "Eval loss: 94.02152252197266\n",
      "Eval Accuracy:0.5963578146888133\n",
      "Epoch975 Training loss: 25.50545883178711\n",
      "Training Accuracy:0.7934975369458128\n",
      "Eval loss: 110.28630065917969\n",
      "Eval Accuracy:0.573944366619972\n",
      "Epoch1000 Training loss: 26.58205223083496\n",
      "Training Accuracy:0.7897536945812808\n",
      "Eval loss: 116.322509765625\n",
      "Eval Accuracy:0.573944366619972\n",
      "Epoch1025 Training loss: 24.0394344329834\n",
      "Training Accuracy:0.8015763546798029\n",
      "Eval loss: 122.13278198242188\n",
      "Eval Accuracy:0.580148088853312\n",
      "Epoch1050 Training loss: 24.093433380126953\n",
      "Training Accuracy:0.8001970443349754\n",
      "Eval loss: 118.75352478027344\n",
      "Eval Accuracy:0.5957574544726836\n",
      "Epoch1075 Training loss: 23.240577697753906\n",
      "Training Accuracy:0.8065024630541872\n",
      "Eval loss: 104.2111587524414\n",
      "Eval Accuracy:0.6245747448469081\n",
      "Epoch1100 Training loss: 22.902746200561523\n",
      "Training Accuracy:0.8110344827586207\n",
      "Eval loss: 136.72023010253906\n",
      "Eval Accuracy:0.573143886331799\n",
      "Epoch1125 Training loss: 25.391311645507812\n",
      "Training Accuracy:0.7919211822660098\n",
      "Eval loss: 105.6627197265625\n",
      "Eval Accuracy:0.6009605763458075\n",
      "Epoch1150 Training loss: 25.74443244934082\n",
      "Training Accuracy:0.7911330049261084\n",
      "Eval loss: 103.41739654541016\n",
      "Eval Accuracy:0.5877526515909546\n",
      "Epoch1175 Training loss: 22.920352935791016\n",
      "Training Accuracy:0.8078817733990148\n",
      "Eval loss: 114.7044906616211\n",
      "Eval Accuracy:0.5957574544726836\n",
      "Epoch1200 Training loss: 22.213376998901367\n",
      "Training Accuracy:0.811231527093596\n",
      "Eval loss: 110.19692993164062\n",
      "Eval Accuracy:0.5931558935361216\n",
      "Epoch1225 Training loss: 22.93463706970215\n",
      "Training Accuracy:0.8175369458128079\n",
      "Eval loss: 113.98707580566406\n",
      "Eval Accuracy:0.5809485691414848\n",
      "Epoch1250 Training loss: 22.55556297302246\n",
      "Training Accuracy:0.8147783251231527\n",
      "Eval loss: 106.24072265625\n",
      "Eval Accuracy:0.6091654992995797\n",
      "Epoch1275 Training loss: 23.766157150268555\n",
      "Training Accuracy:0.8108374384236453\n",
      "Eval loss: 101.47908782958984\n",
      "Eval Accuracy:0.6033620172103262\n",
      "Epoch1300 Training loss: 24.300283432006836\n",
      "Training Accuracy:0.8015763546798029\n",
      "Eval loss: 101.29399108886719\n",
      "Eval Accuracy:0.6349809885931559\n",
      "Epoch1325 Training loss: 22.552289962768555\n",
      "Training Accuracy:0.8189162561576354\n",
      "Eval loss: 103.69147491455078\n",
      "Eval Accuracy:0.6221733039823895\n",
      "Epoch1350 Training loss: 23.473506927490234\n",
      "Training Accuracy:0.814384236453202\n",
      "Eval loss: 113.5181655883789\n",
      "Eval Accuracy:0.5939563738242946\n",
      "Epoch1375 Training loss: 23.49170684814453\n",
      "Training Accuracy:0.8076847290640394\n",
      "Eval loss: 103.84575653076172\n",
      "Eval Accuracy:0.6073644186511907\n",
      "Epoch1400 Training loss: 22.431415557861328\n",
      "Training Accuracy:0.8228571428571428\n",
      "Eval loss: 109.01670837402344\n",
      "Eval Accuracy:0.5995597358415049\n",
      "Epoch1425 Training loss: 22.87740135192871\n",
      "Training Accuracy:0.8161576354679803\n",
      "Eval loss: 109.76573944091797\n",
      "Eval Accuracy:0.6099659795877527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1450 Training loss: 22.867382049560547\n",
      "Training Accuracy:0.8086699507389162\n",
      "Eval loss: 108.82743072509766\n",
      "Eval Accuracy:0.5941564938963378\n",
      "Epoch1475 Training loss: 22.27448081970215\n",
      "Training Accuracy:0.8199014778325123\n",
      "Eval loss: 126.30577087402344\n",
      "Eval Accuracy:0.5711426856113668\n",
      "Epoch1500 Training loss: 22.885955810546875\n",
      "Training Accuracy:0.81064039408867\n",
      "Eval loss: 111.17328643798828\n",
      "Eval Accuracy:0.5935561336802081\n",
      "Epoch1525 Training loss: 22.35572052001953\n",
      "Training Accuracy:0.8124137931034483\n",
      "Eval loss: 107.14436340332031\n",
      "Eval Accuracy:0.6169701821092656\n",
      "Epoch1550 Training loss: 23.127561569213867\n",
      "Training Accuracy:0.8074876847290641\n",
      "Eval loss: 110.12303924560547\n",
      "Eval Accuracy:0.6163698218931358\n",
      "Epoch1575 Training loss: 22.603683471679688\n",
      "Training Accuracy:0.8175369458128079\n",
      "Eval loss: 102.8823471069336\n",
      "Eval Accuracy:0.6245747448469081\n",
      "Epoch1600 Training loss: 23.95785903930664\n",
      "Training Accuracy:0.8096551724137931\n",
      "Eval loss: 115.81414031982422\n",
      "Eval Accuracy:0.6017610566339804\n",
      "Epoch1625 Training loss: 21.186573028564453\n",
      "Training Accuracy:0.8269950738916256\n",
      "Eval loss: 106.54533386230469\n",
      "Eval Accuracy:0.6337802681608965\n",
      "Epoch1650 Training loss: 21.774471282958984\n",
      "Training Accuracy:0.8193103448275862\n",
      "Eval loss: 127.89259338378906\n",
      "Eval Accuracy:0.5839503702221333\n",
      "Epoch1675 Training loss: 23.765850067138672\n",
      "Training Accuracy:0.8074876847290641\n",
      "Eval loss: 100.18896484375\n",
      "Eval Accuracy:0.6091654992995797\n",
      "Epoch1700 Training loss: 22.791902542114258\n",
      "Training Accuracy:0.8175369458128079\n",
      "Eval loss: 112.23968505859375\n",
      "Eval Accuracy:0.611166700020012\n",
      "Epoch1725 Training loss: 22.51369285583496\n",
      "Training Accuracy:0.8151724137931035\n",
      "Eval loss: 103.05243682861328\n",
      "Eval Accuracy:0.6117670602361417\n",
      "Epoch1750 Training loss: 22.10728645324707\n",
      "Training Accuracy:0.8145812807881774\n",
      "Eval loss: 104.3821792602539\n",
      "Eval Accuracy:0.5755453271963178\n",
      "Epoch1775 Training loss: 22.05963134765625\n",
      "Training Accuracy:0.8208866995073891\n",
      "Eval loss: 97.828369140625\n",
      "Eval Accuracy:0.6289773864318591\n",
      "Epoch1800 Training loss: 22.36097526550293\n",
      "Training Accuracy:0.8204926108374384\n",
      "Eval loss: 105.01408386230469\n",
      "Eval Accuracy:0.6249749849909946\n",
      "Epoch1825 Training loss: 22.222408294677734\n",
      "Training Accuracy:0.8199014778325123\n",
      "Eval loss: 104.94659423828125\n",
      "Eval Accuracy:0.6129677806684011\n",
      "Epoch1850 Training loss: 22.606840133666992\n",
      "Training Accuracy:0.8199014778325123\n",
      "Eval loss: 105.39139556884766\n",
      "Eval Accuracy:0.6395837502501501\n",
      "Epoch1875 Training loss: 24.803741455078125\n",
      "Training Accuracy:0.7998029556650247\n",
      "Eval loss: 93.74929809570312\n",
      "Eval Accuracy:0.6131679007404442\n",
      "Epoch1900 Training loss: 22.934194564819336\n",
      "Training Accuracy:0.8155665024630542\n",
      "Eval loss: 128.77244567871094\n",
      "Eval Accuracy:0.5991594956974184\n",
      "Epoch1925 Training loss: 21.3726806640625\n",
      "Training Accuracy:0.8244334975369458\n",
      "Eval loss: 96.07091522216797\n",
      "Eval Accuracy:0.6403842305383229\n",
      "Epoch1950 Training loss: 22.164161682128906\n",
      "Training Accuracy:0.8169458128078818\n",
      "Eval loss: 104.55886840820312\n",
      "Eval Accuracy:0.6309785871522914\n",
      "Epoch1975 Training loss: 21.860332489013672\n",
      "Training Accuracy:0.8262068965517242\n",
      "Eval loss: 101.12635803222656\n",
      "Eval Accuracy:0.6319791875125075\n",
      "Epoch2000 Training loss: 22.090845108032227\n",
      "Training Accuracy:0.8226600985221675\n",
      "Eval loss: 98.60330200195312\n",
      "Eval Accuracy:0.6365819491695017\n",
      "Epoch2025 Training loss: 23.630817413330078\n",
      "Training Accuracy:0.8157635467980295\n",
      "Eval loss: 98.6558837890625\n",
      "Eval Accuracy:0.6381829097458475\n",
      "Epoch2050 Training loss: 22.70771026611328\n",
      "Training Accuracy:0.8155665024630542\n",
      "Eval loss: 110.36427307128906\n",
      "Eval Accuracy:0.6001600960576345\n",
      "Epoch2075 Training loss: 23.686174392700195\n",
      "Training Accuracy:0.8057142857142857\n",
      "Eval loss: 110.281005859375\n",
      "Eval Accuracy:0.6133680208124875\n",
      "Epoch2100 Training loss: 23.83261489868164\n",
      "Training Accuracy:0.8033497536945813\n",
      "Eval loss: 107.10321044921875\n",
      "Eval Accuracy:0.6293776265759455\n",
      "Epoch2125 Training loss: 26.3394775390625\n",
      "Training Accuracy:0.7824630541871921\n",
      "Eval loss: 103.55626678466797\n",
      "Eval Accuracy:0.6251751050630379\n",
      "Epoch2150 Training loss: 22.7174072265625\n",
      "Training Accuracy:0.8206896551724138\n",
      "Eval loss: 110.61150360107422\n",
      "Eval Accuracy:0.5827496497898739\n",
      "Epoch2175 Training loss: 22.517658233642578\n",
      "Training Accuracy:0.8216748768472907\n",
      "Eval loss: 103.26596069335938\n",
      "Eval Accuracy:0.6317790674404643\n",
      "Epoch2200 Training loss: 23.682247161865234\n",
      "Training Accuracy:0.8139901477832512\n",
      "Eval loss: 103.94894409179688\n",
      "Eval Accuracy:0.6355813488092855\n",
      "Epoch2225 Training loss: 22.26304817199707\n",
      "Training Accuracy:0.8151724137931035\n",
      "Eval loss: 115.0428466796875\n",
      "Eval Accuracy:0.6237742645587352\n",
      "Epoch2250 Training loss: 23.61161994934082\n",
      "Training Accuracy:0.8120197044334976\n",
      "Eval loss: 117.7339096069336\n",
      "Eval Accuracy:0.6065639383630178\n",
      "Epoch2275 Training loss: 23.954151153564453\n",
      "Training Accuracy:0.8084729064039409\n",
      "Eval loss: 108.96537780761719\n",
      "Eval Accuracy:0.6075645387232339\n",
      "Epoch2300 Training loss: 22.737590789794922\n",
      "Training Accuracy:0.8114285714285714\n",
      "Eval loss: 100.93211364746094\n",
      "Eval Accuracy:0.6401841104662798\n",
      "Epoch2325 Training loss: 21.220794677734375\n",
      "Training Accuracy:0.8234482758620689\n",
      "Eval loss: 105.18557739257812\n",
      "Eval Accuracy:0.6283770262157294\n",
      "Epoch2350 Training loss: 22.34739112854004\n",
      "Training Accuracy:0.8212807881773398\n",
      "Eval loss: 108.00129699707031\n",
      "Eval Accuracy:0.6113668200920552\n",
      "Epoch2375 Training loss: 26.172571182250977\n",
      "Training Accuracy:0.7814778325123153\n",
      "Eval loss: 96.10552978515625\n",
      "Eval Accuracy:0.6125675405243146\n",
      "Epoch2400 Training loss: 25.82898712158203\n",
      "Training Accuracy:0.7864039408866995\n",
      "Eval loss: 101.59941101074219\n",
      "Eval Accuracy:0.6311787072243346\n",
      "Epoch2425 Training loss: 26.04535484313965\n",
      "Training Accuracy:0.7877832512315271\n",
      "Eval loss: 91.7452163696289\n",
      "Eval Accuracy:0.6205723434060436\n",
      "Epoch2450 Training loss: 25.000242233276367\n",
      "Training Accuracy:0.798423645320197\n",
      "Eval loss: 103.1932601928711\n",
      "Eval Accuracy:0.6381829097458475\n",
      "Epoch2475 Training loss: 24.018468856811523\n",
      "Training Accuracy:0.7948768472906403\n",
      "Eval loss: 99.45439147949219\n",
      "Eval Accuracy:0.6149689813888333\n",
      "Epoch2500 Training loss: 24.022071838378906\n",
      "Training Accuracy:0.8059113300492611\n",
      "Eval loss: 100.15155792236328\n",
      "Eval Accuracy:0.6361817090254153\n",
      "Epoch2525 Training loss: 22.861751556396484\n",
      "Training Accuracy:0.8102463054187192\n",
      "Eval loss: 110.14513397216797\n",
      "Eval Accuracy:0.6305783470082049\n",
      "Epoch2550 Training loss: 23.57461929321289\n",
      "Training Accuracy:0.8043349753694581\n",
      "Eval loss: 93.39240264892578\n",
      "Eval Accuracy:0.6732039223534121\n",
      "Epoch2575 Training loss: 24.88347625732422\n",
      "Training Accuracy:0.8009852216748768\n",
      "Eval loss: 130.3584747314453\n",
      "Eval Accuracy:0.5901540924554732\n",
      "Epoch2600 Training loss: 26.741872787475586\n",
      "Training Accuracy:0.7852216748768472\n",
      "Eval loss: 95.17977142333984\n",
      "Eval Accuracy:0.6331799079447669\n",
      "Epoch2625 Training loss: 23.376462936401367\n",
      "Training Accuracy:0.8074876847290641\n",
      "Eval loss: 107.14091491699219\n",
      "Eval Accuracy:0.6309785871522914\n",
      "Epoch2650 Training loss: 25.011560440063477\n",
      "Training Accuracy:0.7988177339901478\n",
      "Eval loss: 116.82733917236328\n",
      "Eval Accuracy:0.600560336201721\n",
      "Epoch2675 Training loss: 21.72731590270996\n",
      "Training Accuracy:0.8206896551724138\n",
      "Eval loss: 104.81000518798828\n",
      "Eval Accuracy:0.6461877126275766\n",
      "Epoch2700 Training loss: 23.347570419311523\n",
      "Training Accuracy:0.8078817733990148\n",
      "Eval loss: 119.22904205322266\n",
      "Eval Accuracy:0.600560336201721\n",
      "Epoch2725 Training loss: 23.793920516967773\n",
      "Training Accuracy:0.8082758620689655\n",
      "Eval loss: 139.81912231445312\n",
      "Eval Accuracy:0.5469281568941364\n",
      "Epoch2750 Training loss: 21.595623016357422\n",
      "Training Accuracy:0.8273891625615764\n",
      "Eval loss: 105.5317611694336\n",
      "Eval Accuracy:0.6337802681608965\n",
      "Epoch2775 Training loss: 24.708444595336914\n",
      "Training Accuracy:0.7994088669950739\n",
      "Eval loss: 112.83850860595703\n",
      "Eval Accuracy:0.656393836301781\n",
      "Epoch2800 Training loss: 23.546083450317383\n",
      "Training Accuracy:0.8072906403940887\n",
      "Eval loss: 118.63713073730469\n",
      "Eval Accuracy:0.6485891534920952\n",
      "Epoch2825 Training loss: 23.64250373840332\n",
      "Training Accuracy:0.8068965517241379\n",
      "Eval loss: 101.14695739746094\n",
      "Eval Accuracy:0.6547928757254352\n",
      "Epoch2850 Training loss: 23.642480850219727\n",
      "Training Accuracy:0.8098522167487685\n",
      "Eval loss: 104.28079223632812\n",
      "Eval Accuracy:0.6519911947168301\n",
      "Epoch2875 Training loss: 21.696075439453125\n",
      "Training Accuracy:0.8189162561576354\n",
      "Eval loss: 106.70549774169922\n",
      "Eval Accuracy:0.6379827896738043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2900 Training loss: 22.710254669189453\n",
      "Training Accuracy:0.8100492610837439\n",
      "Eval loss: 100.52517700195312\n",
      "Eval Accuracy:0.6521913147888734\n",
      "Epoch2925 Training loss: 26.58837890625\n",
      "Training Accuracy:0.7917241379310345\n",
      "Eval loss: 115.9793701171875\n",
      "Eval Accuracy:0.5437262357414449\n",
      "Epoch2950 Training loss: 24.181312561035156\n",
      "Training Accuracy:0.8055172413793104\n",
      "Eval loss: 117.44644927978516\n",
      "Eval Accuracy:0.5903542125275165\n",
      "Epoch2975 Training loss: 21.181730270385742\n",
      "Training Accuracy:0.8267980295566503\n",
      "Eval loss: 122.10824584960938\n",
      "Eval Accuracy:0.6225735441264759\n",
      "The best epoch:2919 with an accuracy of 0.6778066840104062\n"
     ]
    }
   ],
   "source": [
    "best_model = train(3000, model,0.1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pred and majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model(data_valid.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(y_pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.cpu().numpy()\n",
    "y_pred_majority = np.array([])\n",
    "for i in range(160):\n",
    "    index_of_int = t_valid_labels == i\n",
    "    counts = np.bincount(pred[index_of_int].astype(int))\n",
    "    y_pred_majority = np.append(y_pred_majority,np.argmax(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using the TA formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7548387096774193\n"
     ]
    }
   ],
   "source": [
    "ecgIdAccuracy = recall_score(valid_labels[:,-1], y_pred_majority, average='macro')\n",
    "adjustementTerm = 1.0 / len(np.unique(valid_labels[:,-1]))\n",
    "ecgIdAccuracy = (ecgIdAccuracy - adjustementTerm) / (1 - adjustementTerm)\n",
    "if ecgIdAccuracy < 0:\n",
    "    ecgIdAccuracy = 0.0\n",
    "print(ecgIdAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the convolution on the average window instead of all window was tested also\n",
    "Better result are obtained by using all the window and doing a majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " however they both fell short of a PCA + LDA classificateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = train_labels[:,-1]\n",
    "labels_valid = valid_labels[:,-1]\n",
    "\n",
    "shuffle1 = np.random.permutation(len(Avg_template_train))\n",
    "train_shuffle = Avg_template_train[shuffle1]\n",
    "TrainLabel_shuffle = labels_train[shuffle1]\n",
    "shuffle2 = np.random.permutation(len(Avg_template_valid))\n",
    "valid_shuffle = Avg_template_valid[shuffle2]\n",
    "ValidLabel_shuffle = labels_valid[shuffle2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca = PCA(n_components=30)\n",
    "ipca.fit(train_shuffle)\n",
    "pca_train = ipca.transform(train_shuffle)\n",
    "pca_valid = ipca.transform(valid_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78125"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(pca_train, TrainLabel_shuffle)\n",
    "y_pred = lda.predict(pca_valid)\n",
    "lda.score(pca_valid, ValidLabel_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7741935483870968\n"
     ]
    }
   ],
   "source": [
    "ecgIdAccuracy = recall_score(ValidLabel_shuffle, y_pred, average='macro')\n",
    "adjustementTerm = 1.0 / len(np.unique(ValidLabel_shuffle))\n",
    "ecgIdAccuracy = (ecgIdAccuracy - adjustementTerm) / (1 - adjustementTerm)\n",
    "if ecgIdAccuracy < 0:\n",
    "    ecgIdAccuracy = 0.0\n",
    "print(ecgIdAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding the hearth beat information it does not add anything.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train = np.concatenate((pca_train,np.expand_dims(np.array(Train_RR_mean), axis = 1)[shuffle1]),axis = 1)\n",
    "pca_train = np.concatenate((pca_train,np.expand_dims(np.array(Train_RR_std), axis = 1)[shuffle1]),axis = 1)\n",
    "pca_valid = np.concatenate((pca_valid,np.expand_dims(np.array(Valid_RR_mean), axis = 1)[shuffle2]),axis = 1)\n",
    "pca_valid = np.concatenate((pca_valid,np.expand_dims(np.array(Valid_RR_std), axis = 1)[shuffle2]),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78125"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(pca_train, TrainLabel_shuffle)\n",
    "y_pred = lda.predict(pca_valid)\n",
    "lda.score(pca_valid, ValidLabel_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement from adding the hearth beat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Try (code not keep): individually analyse each window with PCA + LDA and do With Majority vote \n",
    "dont gives better than analyse the average window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Task on averaged window with multitask learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = train_labels[:]\n",
    "labels_valid = valid_labels[:]\n",
    "data_train = torch.Tensor(Avg_template_train)\n",
    "labels_train_t = torch.Tensor(labels_train)\n",
    "trainloader = torch.utils.data.TensorDataset(data_train, labels_train_t)\n",
    "loader_train = torch.utils.data.DataLoader(trainloader, batch_size=160, shuffle = False)\n",
    "\n",
    "data_valid = torch.Tensor(Avg_template_valid)\n",
    "labels_valid_t = torch.Tensor(labels_valid)\n",
    "validloader = torch.utils.data.TensorDataset(data_valid, labels_valid_t)\n",
    "loader_valid = torch.utils.data.DataLoader(validloader, batch_size=160, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Reg_Net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,10,kernel_size=3) #22\n",
    "        self.conv2=nn.Conv1d(10,20,kernel_size=3) #18\n",
    "        self.conv3=nn.Conv1d(20,20,kernel_size=3) #18\n",
    "        self.drop_conv3=nn.Dropout2d(p=0.5) #4\n",
    "\n",
    "        \n",
    "        self.fc1RR_std=nn.Linear(60,30)\n",
    "        self.fc2RR_std=nn.Linear(30,1)\n",
    "        self.fc1TR_mean=nn.Linear(60,30)\n",
    "        self.fc2TR_mean=nn.Linear(30,1)\n",
    "        self.fc1PR_mean=nn.Linear(60,30)\n",
    "        self.fc2PR_mean=nn.Linear(30,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x=F.relu(F.max_pool1d(self.conv1(x),3))\n",
    "        x=F.relu(F.max_pool1d(self.conv2(x),3))\n",
    "        x=F.relu(F.max_pool1d(self.drop_conv3(self.conv3(x)),3))\n",
    "        x=x.view(-1,60)\n",
    "        \n",
    "        \n",
    "        xRR_std = F.relu((self.fc1RR_std(x)))\n",
    "        xRR_std = self.fc2RR_std(xRR_std)\n",
    "\n",
    "        \n",
    "        \n",
    "        xTR_mean = F.relu((self.fc1TR_mean(x)))\n",
    "        xTR_mean = self.fc2TR_mean(xTR_mean)\n",
    "\n",
    "        \n",
    "        xPR_mean = F.relu((self.fc1PR_mean(x)))\n",
    "        xPR_mean = self.fc2PR_mean(xPR_mean) \n",
    "\n",
    "    \n",
    "        return xRR_std, xTR_mean, xPR_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = Reg_Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, lr=0.1):\n",
    "  \n",
    "    optmizer=optim.Adam(reg_model.parameters(),lr=lr)\n",
    "    reg_best_model =copy.deepcopy(reg_model)\n",
    "    best_kendall=0\n",
    "    best_epoch=0\n",
    "    criterion = nn.MSELoss()\n",
    "  \n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        for mode in [\"train\",\"eval\"]:\n",
    "\n",
    "            accuracy=0\n",
    "            loss_total=0\n",
    "\n",
    "            if mode == \"train\":\n",
    "\n",
    "                reg_model.train()\n",
    "                for data, labels in loader_train:\n",
    "                    data, labels=data.to(device), labels.to(device)\n",
    "                    optmizer.zero_grad()\n",
    "                    xTR_mean = reg_model(data)\n",
    "\n",
    "                    xRR_std, xTR_mean, xPR_mean=reg_model(data)\n",
    "                    LossRR_std = criterion(xRR_std,labels[:,2].unsqueeze(1))\n",
    "                    LossTR_mean = criterion(xTR_mean,labels[:,1].unsqueeze(1))\n",
    "                    LossPR_mean = criterion(xPR_mean,labels[:,0].unsqueeze(1))\n",
    "                    loss = LossRR_std + LossTR_mean + LossPR_mean\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    \n",
    "                    optmizer.step()\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        kendallTR_mean = kendalltau(xTR_mean.cpu().detach().numpy(), labels[:,1].cpu().detach().numpy())[0]\n",
    "                        kendallRR_std = kendalltau(xRR_std.cpu().detach().numpy(), labels[:,2].cpu().detach().numpy())[0]\n",
    "                        kendallPR_mean = kendalltau(xPR_mean.cpu().detach().numpy(), labels[:,0].cpu().detach().numpy())[0]\n",
    "                        kendallAvg = np.mean([kendallRR_std, kendallTR_mean, kendallPR_mean])\n",
    "\n",
    "                        print(f\"Epoch{i} Training loss: {loss.item()}\")\n",
    "                        print(f\"Training RR STD loss: {LossRR_std.item()}\")\n",
    "                        print(f\"Training TR Mean loss: {LossTR_mean.item()}\")\n",
    "                        print(f\"Training PR Mean loss: {LossPR_mean.item()}\")\n",
    "                        print(f\"Training Kendall TR: {kendallTR_mean}\")\n",
    "                        print(f\"Training Kendall std: {kendallRR_std}\")\n",
    "                        print(f\"Training Kendall PR: {kendallPR_mean}\")\n",
    "                        print(f\"Training Avg Kendall: {kendallAvg}\")\n",
    "\n",
    "\n",
    "\n",
    "            if mode == \"eval\":\n",
    "\n",
    "                reg_model.eval()\n",
    "                for data, labels in loader_valid:\n",
    "                    data, labels=data.to(device), labels.to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        optmizer.zero_grad()\n",
    "                        xTR_mean = reg_model(data)\n",
    "\n",
    "                        xRR_std, xTR_mean, xPR_mean=reg_model(data)\n",
    "                        LossRR_std = criterion(xRR_std,labels[:,2].unsqueeze(1))\n",
    "                        LossTR_mean = criterion(xTR_mean,labels[:,1].unsqueeze(1))\n",
    "                        LossPR_mean = criterion(xPR_mean,labels[:,0].unsqueeze(1))\n",
    "                        loss = 0.1*LossRR_std + LossTR_mean + LossPR_mean\n",
    "                        loss = F.mse_loss(xTR_mean,labels)\n",
    "                        \n",
    "\n",
    "                        loss_total+=loss\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    with torch.no_grad():\n",
    "                        kendallTR_mean = kendalltau(xTR_mean.cpu().detach().numpy(), labels[:,1].cpu().detach().numpy())[0]\n",
    "                        kendallRR_std = kendalltau(xRR_std.cpu().detach().numpy(), labels[:,2].cpu().detach().numpy())[0]\n",
    "                        kendallPR_mean = kendalltau(xPR_mean.cpu().detach().numpy(), labels[:,0].cpu().detach().numpy())[0]\n",
    "                        kendallAvg = np.mean([kendallRR_std, kendallTR_mean, kendallPR_mean])\n",
    "\n",
    "                        print(f\"Epoch{i} Val loss: {loss.item()}\")\n",
    "                        print(f\"Val RR STD loss: {LossRR_std.item()}\")\n",
    "                        print(f\"Val TR Mean loss: {LossTR_mean.item()}\")\n",
    "                        print(f\"Val PR Mean loss: {LossPR_mean.item()}\")\n",
    "                        print(f\"Val Kendall TR: {kendallTR_mean}\")\n",
    "                        print(f\"Val Kendall std: {kendallRR_std}\")\n",
    "                        print(f\"Val Kendall PR: {kendallPR_mean}\")\n",
    "                        print(f\"Val Avg Kendall: {kendallAvg}\")\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "                    if kendallAvg > best_kendall:\n",
    "                        best_kendall = kendallAvg\n",
    "                        reg_best_model=copy.deepcopy(reg_model)\n",
    "                        best_epoch=i\n",
    "\n",
    "    print(f\"The best epoch:{best_epoch} with an avg kendall of {best_kendall}\")\n",
    "\n",
    "    return reg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 Training loss: 1539.9134521484375\n",
      "Training RR STD loss: 44.980445861816406\n",
      "Training TR Mean loss: 1217.284912109375\n",
      "Training PR Mean loss: 277.6480407714844\n",
      "Training Kendall TR: -0.008020129050261433\n",
      "Training Kendall std: -0.0009433962264150943\n",
      "Training Kendall PR: 0.046701784875847654\n",
      "Training Avg Kendall: 0.012579419866390376\n",
      "Epoch0 Val loss: 440.38385009765625\n",
      "Val RR STD loss: 45.070228576660156\n",
      "Val TR Mean loss: 1142.0185546875\n",
      "Val PR Mean loss: 265.08514404296875\n",
      "Val Kendall TR: 0.179902502617629\n",
      "Val Kendall std: 0.15716026585514223\n",
      "Val Kendall PR: 0.31640195390443143\n",
      "Val Avg Kendall: 0.21782157412573422\n",
      "Epoch1000 Training loss: 11.605323791503906\n",
      "Training RR STD loss: 6.517947196960449\n",
      "Training TR Mean loss: 2.6349005699157715\n",
      "Training PR Mean loss: 2.4524760246276855\n",
      "Training Kendall TR: 0.714892287499774\n",
      "Training Kendall std: 0.4122641509433962\n",
      "Training Kendall PR: 0.666719083749475\n",
      "Training Avg Kendall: 0.5979585073975483\n",
      "Epoch1000 Val loss: 241.9695281982422\n",
      "Val RR STD loss: 12.082941055297852\n",
      "Val TR Mean loss: 38.14495086669922\n",
      "Val PR Mean loss: 14.3902006149292\n",
      "Val Kendall TR: 0.4986633180074314\n",
      "Val Kendall std: 0.22823224200974382\n",
      "Val Kendall PR: 0.5464695774442839\n",
      "Val Avg Kendall: 0.4244550458204864\n",
      "Epoch2000 Training loss: 4.749744892120361\n",
      "Training RR STD loss: 2.1770052909851074\n",
      "Training TR Mean loss: 1.1614792346954346\n",
      "Training PR Mean loss: 1.4112603664398193\n",
      "Training Kendall TR: 0.8180531631266661\n",
      "Training Kendall std: 0.618867924528302\n",
      "Training Kendall PR: 0.7478575382812507\n",
      "Training Avg Kendall: 0.7282595419787397\n",
      "Epoch2000 Val loss: 262.824462890625\n",
      "Val RR STD loss: 13.6303071975708\n",
      "Val TR Mean loss: 26.465246200561523\n",
      "Val PR Mean loss: 15.917200088500977\n",
      "Val Kendall TR: 0.6000943618783848\n",
      "Val Kendall std: 0.1664373600877119\n",
      "Val Kendall PR: 0.6780940483279861\n",
      "Val Avg Kendall: 0.481541923431361\n",
      "Epoch3000 Training loss: 3.6644339561462402\n",
      "Training RR STD loss: 1.5008769035339355\n",
      "Training TR Mean loss: 1.0716711282730103\n",
      "Training PR Mean loss: 1.0918861627578735\n",
      "Training Kendall TR: 0.8375530847390664\n",
      "Training Kendall std: 0.7209119496855346\n",
      "Training Kendall PR: 0.8008491258339802\n",
      "Training Avg Kendall: 0.786438053419527\n",
      "Epoch3000 Val loss: 264.1880798339844\n",
      "Val RR STD loss: 14.226460456848145\n",
      "Val TR Mean loss: 24.396587371826172\n",
      "Val PR Mean loss: 15.19403076171875\n",
      "Val Kendall TR: 0.7227551591176773\n",
      "Val Kendall std: 0.15999056579050244\n",
      "Val Kendall PR: 0.7079729604760191\n",
      "Val Avg Kendall: 0.530239561794733\n",
      "Epoch4000 Training loss: 2.839186906814575\n",
      "Training RR STD loss: 1.4632484912872314\n",
      "Training TR Mean loss: 0.7441827058792114\n",
      "Training PR Mean loss: 0.631755530834198\n",
      "Training Kendall TR: 0.869319086075396\n",
      "Training Kendall std: 0.720440251572327\n",
      "Training Kendall PR: 0.8403176376314138\n",
      "Training Avg Kendall: 0.8100256584263789\n",
      "Epoch4000 Val loss: 274.71832275390625\n",
      "Val RR STD loss: 14.052591323852539\n",
      "Val TR Mean loss: 20.123859405517578\n",
      "Val PR Mean loss: 14.12205982208252\n",
      "Val Kendall TR: 0.7606542003159715\n",
      "Val Kendall std: 0.15653131031395107\n",
      "Val Kendall PR: 0.7101745645290319\n",
      "Val Avg Kendall: 0.5424533583863181\n",
      "The best epoch:4000 with an avg kendall of 0.5424533583863181\n"
     ]
    }
   ],
   "source": [
    "reg_model = train(5000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_TR_mean= reg_model(data_valid.to(device)).cpu().detach().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendalltau(y_TR_mean, labels_valid[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_TR_mean, labels_train[:], \"ro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Task Baseline PCA + SVM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = train_labels[:]\n",
    "labels_valid = valid_labels[:]\n",
    "\n",
    "shuffle1 = np.random.permutation(len(Avg_template_train))\n",
    "train_shuffle = Avg_template_train[shuffle1]\n",
    "TrainLabel_shuffle = labels_train[shuffle1]\n",
    "shuffle2 = np.random.permutation(len(Avg_template_valid))\n",
    "valid_shuffle = Avg_template_valid[shuffle2]\n",
    "ValidLabel_shuffle = labels_valid[shuffle2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "ipca = PCA(n_components=30)\n",
    "ipca.fit(train_shuffle)\n",
    "pca_train = ipca.transform(train_shuffle)\n",
    "pca_valid = ipca.transform(valid_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'C': [1, 5, 10, 15, 100, 1000,10000], 'epsilon': [0.001, 0.0001,0.1,0.5,1,10], \"degree\" : [3,4]}]\n",
    "svm_regressor = {}\n",
    "svm = SVR(gamma=\"auto\")\n",
    "result = []\n",
    "for i in range(3):\n",
    "    clf = GridSearchCV(svm, param_grid, cv=5)\n",
    "    clf.fit(pca_train, TrainLabel_shuffle[:,i])\n",
    "    y_pred = clf.predict(pca_valid)\n",
    "    svm_regressor[LABELS_NAME[i]] = copy.deepcopy(clf)\n",
    "    print(LABELS_NAME[i])\n",
    "    result.append(kendalltau(y_pred, ValidLabel_shuffle[:,i])[0])\n",
    "    print(kendalltau(y_pred, ValidLabel_shuffle[:,i])[0])\n",
    "print(\"Avg Kendall corr\") \n",
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot R peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_peak_amplitude = [[preprocess_train[i][int(j)] for j in record] for i, record in enumerate(R_peak_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_look = 10\n",
    "plt.plot(R_peak_train[num_to_look],R_peak_amplitude[num_to_look],\"ro\")\n",
    "Plot_ECG(preprocess_train[num_to_look].cpu(),start=0,end=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RR std pred vs real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Train_RR_std, train_labels[:,-2], \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendalltau(Train_RR_std, train_labels[:,-2])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the position of the other points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_numpy = preprocess_train.cpu().numpy()\n",
    "R_peak = R_peak_train\n",
    "RR_interval_mean = Train_RR_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code below note that its the same thing for the T and P peak (we only pass the reverse list for the P peak) so the code could be improve and cut in function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoftemplate = []\n",
    "listofQRS_offset = []\n",
    "listofT_peak = []\n",
    "listofT_offset = []\n",
    "listofQRS_onset = []\n",
    "listofP_peak = []\n",
    "listofP_onset = []\n",
    "listofR_Peak = []\n",
    "\n",
    "for recording in range(len(R_peak)):\n",
    "    #generate the template\n",
    "    half_size_int = int(RR_interval_mean[recording]//2)\n",
    "    template = np.ones((1,int(half_size_int*0.8)+int(half_size_int*1.2)))\n",
    "    for i in R_peak[recording][1:-1]:\n",
    "        new_heart_beat = S_numpy[recording][int(i)-int(half_size_int*0.8): int(i)+int(half_size_int*1.2)]\n",
    "        template = np.concatenate((template,\n",
    "                                   np.expand_dims(new_heart_beat, axis = 0))\n",
    "                                   , axis=0)\n",
    "    \n",
    "    \n",
    "    template = np.mean(template, axis = 0)\n",
    "    \n",
    "    #find QRS offset\n",
    "    derivative = template[1:] - template[:-1] \n",
    "    TA = np.array([])\n",
    "    TDA = np.array([])\n",
    "    for i in range(int(half_size_int*0.8)+1, int(half_size_int*0.8)+25):\n",
    "        TA = np.append(TA,np.max(template[i:i+4]) - np.min(template[i:i+4])) \n",
    "        TDA = np.append(TDA,np.max(derivative[i:i+4]) - np.min(derivative[i:i+4]))\n",
    "    c1 = 0.1\n",
    "    c2 = 0.1\n",
    "    TT = c1 * (np.max(TA) - np.min(TA)) + np.min(TA)\n",
    "    TD = c2 * (np.max(TDA) - np.min(TDA)) + np.min(TDA)\n",
    "    \n",
    "    for i in range(0, len(TA)):\n",
    "        if TA[i] < TT or TDA[i] < TD:\n",
    "            QRS_offset = i + int(half_size_int*0.8) + 4\n",
    "            break\n",
    "        \n",
    "    #find T peak\n",
    "    T_peak = QRS_offset + np.argmax(template[QRS_offset:])\n",
    "    \n",
    "    #find T wave offset\n",
    "    \n",
    "    k = (template[T_peak] - template[-1])/(T_peak-len(template))\n",
    "    d = template[-1] - k*len(template)\n",
    "    g = k*np.arange(0,len(template)) + d\n",
    "    decision = template - g\n",
    "    T_peak_offset = np.argmin(decision[T_peak:len(template)]) + T_peak\n",
    "    \n",
    "    #reverse the template\n",
    "    reverse_template = np.flip(template)\n",
    "    \n",
    "    #Find the QRS onset\n",
    "    derivative = reverse_template[1:] - reverse_template[:-1] \n",
    "    TA = np.array([])\n",
    "    TDA = np.array([])\n",
    "    for i in range(int(half_size_int*1.2)+1, int(half_size_int*1.2)+25):\n",
    "        TA = np.append(TA,np.max(reverse_template[i:i+4]) - np.min(reverse_template[i:i+4])) \n",
    "        TDA = np.append(TDA,np.max(derivative[i:i+4]) - np.min(derivative[i:i+4]))\n",
    "    c1 = 0.5\n",
    "    c2 = 0.5\n",
    "    TT = c1 * (np.max(TA) - np.min(TA)) + np.min(TA)\n",
    "    TD = c2 * (np.max(TDA) - np.min(TDA)) + np.min(TDA)\n",
    "    \n",
    "    for i in range(0, len(TA)):\n",
    "        if TA[i] < TT or TDA[i] < TD:\n",
    "            QRS_onset = i + int(half_size_int*1.2) + 4\n",
    "            break\n",
    "    \n",
    "    #Find the P peak\n",
    "    P_peak = QRS_onset + np.argmax(reverse_template[QRS_onset:])  \n",
    "    \n",
    "    #Find the P onset\n",
    "    \n",
    "    k = (reverse_template[P_peak] - reverse_template[-1])/(P_peak-len(reverse_template))\n",
    "    d = reverse_template[-1] - k*len(reverse_template)\n",
    "    g = k*np.arange(0,len(reverse_template)) + d\n",
    "    decision = reverse_template - g\n",
    "    P_onset = np.argmin(decision[P_peak:len(reverse_template)]) + P_peak\n",
    "    \n",
    "    \n",
    "    P_peak = len(template)-P_peak-1\n",
    "    QRS_onset = len(template)-QRS_onset-1\n",
    "    P_onset = len(template)-P_onset-1\n",
    "    \n",
    "    \n",
    "    listofR_Peak.append(int(half_size_int*0.8)+1)\n",
    "    listoftemplate.append(template)\n",
    "    listofQRS_offset.append(QRS_offset)\n",
    "    listofT_peak.append(T_peak)\n",
    "    listofT_offset.append(T_peak_offset)\n",
    "    listofQRS_onset.append(QRS_onset)\n",
    "    listofP_peak.append(P_peak)\n",
    "    listofP_onset.append(P_onset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate RT Mean and PR Mean from the points we have found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_Mean = [T_peak-R_peak+1 for T_peak,R_peak in zip(listofT_peak,listofR_Peak)]\n",
    "PR_Mean = [R_peak-P_peak for R_peak,P_peak in zip(listofR_Peak,listofP_peak)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(PR_Mean, train_labels[:,0], \"ro\")\n",
    "plt.title(\"Predicted PR Mean Interval vs True\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendalltau(PR_Mean, train_labels[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kendalltau(RT_Mean, train_labels[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RT_Mean, train_labels[:, 1], \"ro\")\n",
    "plt.title(\"Predicted TR Mean Interval vs True\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation of a window with the points find on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_nb = 53\n",
    "Plot_ECG(listoftemplate[index_nb],0,len(listoftemplate[index_nb]),seconde=False)\n",
    "plt.plot(listofR_Peak[index_nb]-1, listoftemplate[index_nb][listofR_Peak[index_nb]-1], \"o\")\n",
    "plt.plot(listofQRS_offset[index_nb], listoftemplate[index_nb][listofQRS_offset[index_nb]], \"o\")\n",
    "plt.plot(listofT_peak[index_nb], listoftemplate[index_nb][listofT_peak[index_nb]], \"o\")\n",
    "plt.plot(listofT_offset[index_nb], listoftemplate[index_nb][listofT_offset[index_nb]], \"o\")\n",
    "plt.plot(listofQRS_onset[index_nb], listoftemplate[index_nb][listofQRS_onset[index_nb]], \"o\")\n",
    "plt.plot(listofP_peak[index_nb], listoftemplate[index_nb][listofP_peak[index_nb]], \"o\")\n",
    "plt.plot(listofP_onset[index_nb], listoftemplate[index_nb][listofP_onset[index_nb]], \"o\")\n",
    "plt.legend([\"\",\"R\",\"QRS offset\", \"T\", \"T offset\", \"QRS onset\",\"P\", \"P onset\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation of Labels spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizualize_label(label_name=LABELS_NAME, type_data=train_labels):\n",
    "    # Plot histogram of the label to see their distribution\n",
    "    # Not complete \n",
    "    for i, ex in enumerate(label_name):\n",
    "        plt.figure(i*2+1) #to let the index start at 1\n",
    "        plt.title(ex)\n",
    "        plt.hist(train_labels[:,i])\n",
    "        plt.figure(i*2+2)\n",
    "        plt.plot(train_labels[:,-1], train_labels[:,i],\"ro\")\n",
    "    plt.show()\n",
    "    \n",
    "vizualize_label()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
